{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering Patterns in the Russian Housing Market for Analysis and Prediction\n",
    "### 3804ICT Assignment Part I | Forecasting | Trimester 2, 2019\n",
    "\n",
    "Joshua Russell (s5057545) | joshua.russell2@griffithuni.edu.au\n",
    "\n",
    "\n",
    "Joshua Mitchell (s5055278) | joshua.mitchell4@griffithuni.edu.au\n",
    "\n",
    "\n",
    "Hayden Flatley (s5088623) | hayden.flatley@griffithuni.edu.au"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "> Introduce the algorithm/technique in pretty good detail\n",
    "\n",
    "What is forecasting?\n",
    "\n",
    "We want to look at how the market will behave in the future so that we have the opportunity to capitalise on the market's future state. \n",
    "\n",
    "Why we're using exponential smoothing instead of moving average.( our data is based on dates over multiple years )\n",
    "\n",
    "To do this we need to be able to capture patterns within the time series that take place over both long or short time periods. The moving average method only takes into account the trend of the data and thus are only relevant for applications with little volitality and no seasonal pattern. Our data includes the day, month and year for year house transactions and spans over multiple years thus we expect to find and analyse seasonal behaviour.\n",
    "For this we use a method known as holt winters or triple exponential smoothing that has the ability to model seasonality and trend.\n",
    "\n",
    "Simple exponential smoothing\n",
    "\n",
    "Initially we start off with single exponential smoothing that exponentially decreases the influence of past points using the parameter alpha, which is representative of the weighted moving average method. Alpha is the smoothing coefficient that determines the rate of exponentially decreasing influence of historical values. Greater alpha values mean greater rates of decay. The basic equation is:\n",
    "\\begin{equation*}\n",
    "L_t = \\alpha y_{t-1} + (1 - \\alpha)L_{t-1}\n",
    "\\end{equation*}\n",
    "If we expand it becomes a summation of all previously predicted values.\n",
    "\\begin{equation*}\n",
    "L_t = \\alpha\\sum_{i=1}^{t-2}(1-\\alpha)^{i-1}y_{t-i} + (1-\\alpha)^{t-2}L_2\n",
    "\\end{equation*}\n",
    "From these equations we can see that they're reliant on actual data points that to calculate the next point in the series. This is why single exponential smoothing gives poor forecasting results, as we hold the value constant (known as flat prediction) from the last calculated value for points further into the future. This is also why it's referred to as the level component since it offsets our values by a constant amount off from the x-axis. When using SES it's assumed that the time series has no underlying patterns.\n",
    "\n",
    "Double expoenential smoothing\n",
    "\n",
    "Double exponential smoothing introduces the ability to model trend with a new equation and modifies the existing level equation as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_t = \\alpha y_t + (1 - \\alpha)\\hat y_{t-1}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "T_t = \\beta(L_t - L_{t-1}) + (1 - \\beta)T_{t-1}\n",
    "\\end{equation*}\n",
    "\n",
    "The level equation has changed to include the value from the previous trend and level components at t - 1 as our resulting prediction the addition of both level and trend components is:\n",
    "\\begin{equation*}\\hat y_{t+h} = L_t + h T_t\\end{equation*}\n",
    "The term h is a discrete value to determine the number of steps to predict ahead of our current position in time t heading in the direction of our estimated gradient. Our second equation is an estimated of the trend or gradient with emphasis on more recent points by exponentially decaying historic points similarly to our previous single exponential smoothing equation. Now that we have multiple equations we can also choose different variations of joining our components. This includes additive models where we assume the trend component stays constant as the level changes or a multiplicative model where trend changes with the level. We can also add dampening to reduce the influence of trend as we predict further into the future, this becomes useful we don't expect future growth to continue from the end our data. (phi is our dampening parameter).\n",
    "\n",
    "\\begin{equation*}Additive: \\hat y_{t+1} = L_t + h T_t \\end{equation*}\n",
    "\\begin{equation*}Additive Damped: \\hat y_{t+1} = L_t + (\\phi + \\phi^2 + ... + \\phi^h) T_t\\end{equation*}\n",
    "\\begin{equation*}Multiplicative: \\hat y_{t+1} = L_t T_t^h \\end{equation*}\n",
    "\\begin{equation*}Additive Damped: \\hat y_{t+1} = L_t T_t^{(\\phi + \\phi^2 + ... + \\phi^h)}\\end{equation*}\n",
    "\n",
    "Something we must also consider is the estimation of our initial level and trend components, as they don't have past values to draw from. Our implementation simply uses the value for our first data point as the intial level and trend is the average difference between the first 3 points.\n",
    "\n",
    "Holt Winters\n",
    "\n",
    "Finally, we must add a seasonality component to model patterns which occur repeatedly after a certain number of seasonal periods. Our data is set over multiple years, from 2011 to 2016.\n",
    "\n",
    "Additive and multiplicative seasonal models\n",
    "    Additive assumes that the seasonality stays constant while multiplicative assumes that the seasonality is proportional to the local deseasonalised mean level.\n",
    "    \n",
    "Training the model by changing the parameters alpha, beta and gamma. We can do this by minimising the mean square error.\n",
    "\n",
    "What information does it provide?\n",
    "\n",
    "How is it useful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "> Import library modules, load datasets, preprocess datasets for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_df = pd.read_csv(\"../Data/train.csv\")\n",
    "test_df = pd.read_csv(\"../Data/test.csv\")\n",
    "\n",
    "df = pd.concat([train_df, test_df], sort=False)\n",
    "df = df[[\"timestamp\", \"price_doc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp     object\n",
       "price_doc    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.to_datetime(df[\"timestamp\"])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Solution\n",
    "\n",
    "> Implement algorithm/technique using libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Solution\n",
    "> Implement algorithm/technique manually (numpy is okay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Metrics\n",
    "> Provide experimental results on the dataset. Provide a comparison between the library solution and the manual solution. Graphs and tables and text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
